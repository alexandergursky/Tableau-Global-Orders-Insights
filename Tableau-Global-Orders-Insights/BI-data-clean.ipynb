{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing Global-Orders Dataset for Tableau"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This Dataset covers Global Orders for a broad range of companies. It comtains catagorical features as well as numerical. This dataset was chosen for its complexity, it seemed like a challenging dataset that also had enough information to be fruitful.  \n",
    "  \n",
    "2. **Questions I will answer include:**  \n",
    "- What Regions Produce the most goods?\n",
    "- Why might these regions produce their good(s)?\n",
    "- Is there a correlation between priority level in orders and the profit earned?\n",
    "- Is there a correlation for priority level of an orders and the associated shipping cost?\n",
    "- What American Tech companies produce the most?\n",
    "- What products do the American Tech companies specialize in?\n",
    "- What/where is Canon's target market?\n",
    "- How can Canon capitalize on this market?\n",
    "  \n",
    "3. ETL is listed below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Imports, Data, and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode',\n",
      "       'Customer ID', 'Customer Name', 'Segment', 'City', 'State', 'Country',\n",
      "       'Postal Code', 'Market', 'Region', 'Product ID', 'Category',\n",
      "       'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount',\n",
      "       'Profit', 'Shipping Cost', 'Order Priority'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "data = pd.read_csv('/Users/alexandergursky/Local_Repository/Datasets/CSV/Global-Orders.csv', encoding='ISO-8859-1')\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Row_ID', 'Order_ID', 'Order_Date', 'Ship_Date', 'Ship_Mode',\n",
      "       'Customer_ID', 'Customer_Name', 'Segment', 'City', 'State', 'Country',\n",
      "       'Postal_Code', 'Market', 'Region', 'Product_ID', 'Category',\n",
      "       'Sub-Category', 'Product_Name', 'Sales', 'Quantity', 'Discount',\n",
      "       'Profit', 'Shipping_Cost', 'Order_Priority'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "data.columns = data.columns.str.replace(' ', '_')\n",
    "\n",
    "print(data.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Regular Expressions for Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Plantronics CS510 - Over-the-Head monaural Wir...\n",
      "1            Novimex Executive Leather Armchair, Black\n",
      "2                    Nokia Smart Phone, with Caller ID\n",
      "3                       Motorola Smart Phone, Cordless\n",
      "4                       Sharp Wireless Fax, High-Speed\n",
      "Name: Product_Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lets start cleaning up the product names\n",
    "print(data['Product_Name'].head())\n",
    "\n",
    "# selects the column, splits the column once on specified delmiter, takes the first element from this split list.\n",
    "data.Product_Name = data.Product_Name.str.split(',', n=1).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Plantronics CS510 - Over-the-Head monaural Wir...\n",
      "1                   Novimex Executive Leather Armchair\n",
      "2                                    Nokia Smart Phone\n",
      "3                                 Motorola Smart Phone\n",
      "4                                   Sharp Wireless Fax\n",
      "Name: Product_Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Observing that we got rid of the color atributes that followed the ,\n",
    "print(data['Product_Name'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new subset df to get the company names\n",
    "prod_list = pd.DataFrame({'Product_Name' : data.Product_Name.unique()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".str.replace(r'[^\\w\\s]+', '')\n",
    "```\n",
    "This removes all special characters. The regular expression [^\\w\\s]+ matches any non-alphanumeric character that is not a space or underscore, and replaces it with an empty string. The + indicates that one or more occurrences should be replaced. The \\w matches any word character (letter, digit, or underscore), and \\s matches any whitespace character (space, tab, or newline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters\n",
    "prod_list['Product_Name'] = prod_list['Product_Name'].str.replace(r'[^\\w\\s]+', '', regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    ".str.replace(pattern, '', regex=True)\n",
    "```\n",
    "This would remove all special characters from the column_name column. However, in this approach, you are calling the replace() method for each row in the column, which can be slower for larger datasets.\n",
    "```\n",
    ".apply(lambda x: re.sub(pattern, '', x))\n",
    "```\n",
    "Using a lambda function with apply() allows you to apply the pattern to the entire column at once, resulting in a faster execution time. Additionally, it allows for more flexibility in terms of modifying the input before applying the regular expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define regex pattern to remove non-ASCII characters and special characters\n",
    "pattern = r'[^ -~]'\n",
    "\n",
    "# Remove non-ASCII and special characters from the 'Text' column\n",
    "# The regex pattern [^ -~] will match any characters that are not in the printable ASCII range. \n",
    "prod_list['Product_Name'] = prod_list['Product_Name'].apply(lambda x: re.sub(pattern, '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first word before a space\n",
    "companies = prod_list['Product_Name'].str.split(' ', n=1).str[0]\n",
    "\n",
    "# Get the unique values\n",
    "companies = pd.Series(sorted(companies.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Gathering Company Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop company names containing numbers\n",
    "companies = companies[~companies.str.contains('\\d')]\n",
    "\n",
    "companies = pd.DataFrame({'Company_Names' : companies})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 01:34:51.503879: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "\n",
    "# # load the spaCy English language model\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # define a function to extract company names from text\n",
    "# def extract_company_names(text):\n",
    "#     doc = nlp(text)\n",
    "#     company_names = []\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ == 'ORG':\n",
    "#             company_names.append(ent.text)\n",
    "#     return company_names\n",
    "\n",
    "# # apply the function to your column of words\n",
    "# companies['Company_Names1'] = companies['Company_Names'].apply(extract_company_names)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Some Manual Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the column to a text file\n",
    "col_export = companies['Company_Names']\n",
    "\n",
    "col_export.to_csv('output_file1.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text file into a pandas dataframe\n",
    "comp_txt = pd.read_csv('/Users/alexandergursky/Local_Repository/output_file.txt', header=None, names=['Column1'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Reformatting, Cleaning, and Parsing for Companies/Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert values in 'Name' column to proper case\n",
    "comp_txt['Column1'] = comp_txt['Column1'].str.title()\n",
    "\n",
    "# Appending a new company I can see needs to be added\n",
    "comp_txt.loc[len(comp_txt)] = ['Officestar']\n",
    "comp_txt.loc[len(comp_txt)] = ['Ki']\n",
    "\n",
    "# Drop duplicates\n",
    "comp_txt.drop_duplicates(subset=['Column1'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remaining company names I need to fix\n",
    "comp_dict = {\n",
    "    'Motorla' : 'Motorola',\n",
    "    'Peel' : 'Peeloff',  # later, will make a new dict for these\n",
    "    'Neat' : 'Neatdesk',  # later\n",
    "    'Permanent' : 'Perma',\n",
    "    'Poly' : 'Polycom',  # later\n",
    "    'Star' : 'Startechcom',  # later\n",
    "    'Startech' : 'Startechcom',  # later\n",
    "    'Ihome' : 'Apple',\n",
    "    'Euro' : 'Europro',  # later\n",
    "    'High' : 'Highback',  # later\n",
    "    'Hewlettpackard' : 'Hewlett',\n",
    "    'Martinyale' : 'Martin',\n",
    "    'Memo' : 'Memorex',  # later\n",
    "    'Maxelllto' : 'Maxwell',\n",
    "    'Maxellivdr' : 'Maxwell',\n",
    "    'Ki': 'Kitchenaid',\n",
    "}\n",
    "\n",
    "comp_txt['Column1'].replace(comp_dict, inplace=True)\n",
    "\n",
    "# replace any text after 'Logitech' with 'Logitech'\n",
    "comp_txt['Column1'] = comp_txt['Column1'].str.replace(r'Logitech.*', 'Logitech', regex=True)\n",
    "\n",
    "# replace any text after 'Imation' with 'Imation'\n",
    "comp_txt['Column1'] = comp_txt['Column1'].str.replace(r'Imation.*', 'Imation', regex=True)\n",
    "\n",
    "# Drop duplicates\n",
    "comp_txt.drop_duplicates(subset=['Column1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters\n",
    "data['Product_Name'] = data['Product_Name'].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
    "\n",
    "# Define regex pattern to remove non-ASCII characters and special characters\n",
    "pattern = r'[^ -~]'\n",
    "\n",
    "# Remove non-ASCII and special characters from the 'Product_Name' column\n",
    "# The regex pattern [^ -~] will match any characters that are not in the printable ASCII range. \n",
    "data['Product_Name'] = data['Product_Name'].apply(lambda x: re.sub(pattern, '', x))\n",
    "\n",
    "# Fixing company name in product\n",
    "data['Product_Name'] = data['Product_Name'].str.lower()\n",
    "data['Product_Name'] = data['Product_Name'].replace({'office star' : 'Officestar'}, regex=True)\n",
    "\n",
    "# convert values in 'Product_Name' column to proper case\n",
    "data['Product_Name'] = data['Product_Name'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fix_names_dict = {\n",
    "    'Peel' : 'Peeloff',\n",
    "    'Neat' : 'Neatdesk',\n",
    "    'Poly' : 'Polycom',\n",
    "    'Star' : 'Startechcom',\n",
    "    'Startech' : 'Startechcom',\n",
    "    'Euro' : 'Europro',\n",
    "    'High' : 'Highback',\n",
    "    'Memo' : 'Memorex',\n",
    "}\n",
    "\n",
    "# apply replacement using regex\n",
    "data['Product_Name'] = data['Product_Name'].replace(fix_names_dict, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to match product names with company names\n",
    "def match_company(product_name):\n",
    "    for company in comp_txt['Column1']:\n",
    "        if product_name.startswith(company):\n",
    "            return company\n",
    "    return None\n",
    "\n",
    "# apply the match_company function to the product names and create a new column with the matching company names\n",
    "data['Company_Name'] = data['Product_Name'].apply(match_company)\n",
    "\n",
    "replace_exces = {\n",
    "    'Memorexrex' : 'Memorex',\n",
    "    'Polycomcom' : 'Polycom',\n",
    "}\n",
    "# apply replacement using regex\n",
    "data['Product_Name'] = data['Product_Name'].replace(replace_exces, regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Profit.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed the profit values were incorrect by handcalculating so im fixing it here\n",
    "data['Profit'] = abs(data['Profit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Parsing/Removal of Company Names from Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of company names from the 'Value' column\n",
    "companies = data['Company_Name'].tolist()\n",
    "\n",
    "# Defining a custom function to remove the company name from the 'Product_Name' column if it appears in the 'Company_Name' column\n",
    "def remove_company_name(row, companies):\n",
    "    words = row['Product_Name'].split()\n",
    "    for company in companies:\n",
    "        if company in words:\n",
    "            words.remove(company)\n",
    "            break\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply the custom function to the DataFrame to remove the company name from the 'Product_Name' column if it appears in the 'Company_Name' column\n",
    "data['Product_Name'] = data.apply(lambda row: remove_company_name(row, companies), axis=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning: Removing Unwanted Values (NaN's and NPC's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop naN values\n",
    "data = data.dropna(subset=['Company_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "city, state = pd.DataFrame({'City' : data.City.unique()}), pd.DataFrame({'State' : data.State.unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of spaces in the \"value\" column\n",
    "city['num_spaces'] = city['City'].apply(lambda x: x.count(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City          So Jos dos Campos\n",
      "num_spaces                      3\n",
      "Name: 2087, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(city.iloc[2087])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if a string contains non-printable or non-ASCII characters\n",
    "def has_non_printable_characters(string):\n",
    "    return bool(re.search(r'[^\\x00-\\x7F]', string))\n",
    "\n",
    "# Apply the function to each row of the dataframe and store the result in a new column\n",
    "city['has_non_printable_characters'] = city['City'].apply(has_non_printable_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Boolean mask based on the function\n",
    "mask = city['City'].apply(has_non_printable_characters)\n",
    "\n",
    "# drop rows where the mask is True\n",
    "df = city.drop(index=city[mask].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying this to the dataframe now\n",
    "\n",
    "# create a Boolean mask based on the function\n",
    "mask = data['City'].apply(has_non_printable_characters)\n",
    "mask1 = data['State'].apply(has_non_printable_characters)\n",
    "mask2 = data['Country'].apply(has_non_printable_characters)\n",
    "\n",
    "# drop rows where the mask is True\n",
    "data = data.drop(index=data[mask].index)\n",
    "data = data.drop(index=data[mask1].index)\n",
    "data = data.drop(index=data[mask2].index)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('Row_ID', axis=1)\n",
    "data.to_csv('preprocessed-Global-Orders.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
